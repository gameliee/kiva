{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DEVICES = \"1,2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cudf\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# only create cuda cluster when using dask_cudf, un comment if needed\n",
    "cluster = LocalCUDACluster(\n",
    "    CUDA_VISIBLE_DEVICES=DEVICES, memory_limit=\"48GiB\", device_memory_limit=\"auto\", n_workers=None\n",
    ")\n",
    "# cluster = LocalCluster(n_workers=8, memory_limit=\"48GiB\")\n",
    "\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw data\n",
    "First, read data in `.jsonl` file format as a pandas data frame\n",
    "Then store the dataframe in `.parquet` format for easy access later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df = pd.read_json(\"../fulldata/kiva_activity_2023-08-28T11-09-39.jsonl\", lines=True)\n",
    "df = pd.json_normalize(df[\"loan\"], sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df[\"loanAmount\"] = df[\"loanAmount\"].astype(float)\n",
    "df[\"loanFundraisingInfo_fundedAmount\"] = df[\"loanFundraisingInfo_fundedAmount\"].astype(float)\n",
    "df[\"raisedDate\"] = pd.to_datetime(df[\"raisedDate\"])\n",
    "df[\"fundraisingDate\"] = pd.to_datetime(df[\"fundraisingDate\"])\n",
    "df[\"geocode_country_name\"] = df[\"geocode_country_name\"].astype(\"category\")\n",
    "df[\"sector_id\"] = df[\"sector_id\"].astype(int)\n",
    "df[\"sector_name\"] = df[\"sector_name\"].astype(\"category\")\n",
    "df[\"activity_id\"] = df[\"activity_id\"].astype(int)\n",
    "df[\"activity_name\"] = df[\"activity_name\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "df.to_parquet(\"../fulldata/kiva_activity_2023-08-28T11-09-39.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = cudf.read_parquet(\"../fulldata/kiva_activity_2023-08-28T11-09-39.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rename(columns={\"id\": \"project_id\", \"name\": \"project_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dropna(axis=0, how=\"all\", inplace=True)\n",
    "ds.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data for Vietnam only\n",
    "Filtering, only take `Vietnam` into account\n",
    "Why? Because there are a lot of rows and we try to localize the task\n",
    "ds[\"geocode_country_name\"].value_counts()[\"Vietnam\"]\n",
    "ds = ds[ds[\"geocode_country_name\"] == \"Vietnam\"]\n",
    "Try to limit the timeline, because I am not yet comfortable to work with large data\n",
    "ds = ds[ds.fundraisingDate > \"2020-01-01\"]\n",
    "\"the number of Loans (might duplicated) under investigation is\", len(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicated `loan`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are loans which have a same `id` but different `fundedAmount`\n",
    "It might because the query time is different\n",
    "Here, only keep records which have the highest `fundedAmount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.loc[[9628, 1366545]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ds.groupby(\"id\", group_keys=False)[[\"loanFundraisingInfo_fundedAmount\"]].idxmax()\n",
    "iloc = temp[\"loanFundraisingInfo_fundedAmount\"].values  # NOTE: just iloc, not loc\n",
    "ds = ds.iloc[iloc]\n",
    "del iloc\n",
    "del temp\n",
    "ds.loc[[9628, 1366545]]  # see, only keep the one with higher fundedAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[ds.duplicated(subset=[\"id\"], keep=False)].sort_values(by=[\"id\"])  # no duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"the number of Loans (no duplicated) under investigation is\", len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"sector_name\"] = ds[\"sector_name\"].astype(\"category\")\n",
    "ds[\"geocode_country_name\"] = ds[\"geocode_country_name\"].astype(\"category\")\n",
    "ds[\"activity_name\"] = ds[\"activity_name\"].astype(\"category\")\n",
    "ds[\"sector_id\"] = ds[\"sector_id\"].astype(\"uint8\")\n",
    "ds[\"activity_id\"] = ds[\"activity_id\"].astype(\"uint8\")\n",
    "ds[\"lendingActions_totalCount\"] = ds[\"lendingActions_totalCount\"].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == ds.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create `lender-loan-tag` df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode the `lendingActions_values`, then do dict processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the explosion will require more memmory, the nature choice is to move forward to dask_cudf.  \n",
    "But here, we must process the `dict` data type later.  \n",
    "Hence, just use pandas would be the easiest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()  # slow, about 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode(\"lendingActions_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"lendingActions_values\"], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lender_id\"] = df.lendingActions_values.progress_apply(lambda x: x[\"lender\"][\"id\"]).astype(int)\n",
    "df[\"lender_name\"] = df.lendingActions_values.progress_apply(lambda x: x[\"lender\"][\"name\"])\n",
    "df[\"lender_publicId\"] = df.lendingActions_values.progress_apply(lambda x: x[\"lender\"][\"publicId\"])\n",
    "df[\"loan_shareAmount\"] = df.lendingActions_values.progress_apply(lambda x: x[\"shareAmount\"]).astype(float)\n",
    "df[\"loan_date\"] = pd.to_datetime(df.lendingActions_values.progress_apply(lambda x: x[\"latestSharePurchaseDate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"lendingActions_values\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a column call `loan_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.duplicates(subset=[['project_id', 'lender_id', 'lender_name', 'lender_publicId', 'loan_shareAmount', 'loan_date']])\n",
    "assert 0 == df.duplicated(subset=[\"project_id\", \"lender_id\", \"loan_shareAmount\", \"loan_date\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loan_id\"] = np.arange(len(df[\"lender_id\"])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuDF do not work with timezone yet\n",
    "df[\"loan_date\"] = df[\"loan_date\"].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, expode the `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode(\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"checkpoints/explodeddata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:1000].to_parquet(\"checkpoints/explodeddata_sample_1000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"checkpoints/explodeddata_sample_1000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"checkpoints/explodeddata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Projects\n",
    "df.project_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Lenders\n",
    "df.lender_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many project that doesn't have tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_tags_df = df[[\"project_id\", \"fundraisingDate\", \"tags\"]].drop_duplicates()\n",
    "print(len(project_tags_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result\n",
    "monthly_grouper = pd.Grouper(key=\"fundraisingDate\", freq=\"30d\")\n",
    "perMonth = project_tags_df.groupby(monthly_grouper).agg({\"project_id\": \"nunique\", \"tags\": \"count\"})\n",
    "perMonth.rename(columns={\"project_id\": \"project_count\", \"tags\": \"tag_count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perMonth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perMonth.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_empty_tags = project_tags_df.groupby(\"project_id\").tags.count() == 0\n",
    "project_empty_tags = project_empty_tags.rename(\"is_empty_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_empty_tags = (\n",
    "    project_tags_df.drop(columns=[\"tags\"])\n",
    "    .drop_duplicates()\n",
    "    .merge(project_empty_tags, left_on=\"project_id\", right_index=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_per_month = project_empty_tags.groupby(monthly_grouper).agg({\"project_id\": \"nunique\", \"is_empty_tags\": \"sum\"})\n",
    "empty_per_month.rename(columns={\"project_id\": \"count_project\", \"is_empty_tags\": \"count_project_no_tag\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_per_month[\"percentage_no_tag\"] = empty_per_month[\"count_project_no_tag\"] / empty_per_month[\"count_project\"] * 100\n",
    "empty_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_per_month[[\"percentage_no_tag\"]].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we could say that before 2013, projects do not have tags. Tags appeared since 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag preprocessing without dask\n",
    "\n",
    "Here, we just using pandas, because `cudf` cannot load the big parquet file  \n",
    "Becareful, need 3 hours to do the deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_parquet(\"checkpoints/explodeddata.parquet\")\n",
    "# ds = cudf.read_parquet(\"checkpoints/explodeddata.parquet\")\n",
    "# ds = cudf.read_parquet(\"checkpoints/explodeddata_sample_1000.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tag call `empty`\n",
    "ds[\"tags\"] = ds[\"tags\"].fillna(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"tags\"] = ds.tags.replace([\"user_favorite\", \"user_like\", \"volunteer_like\", \"volunteer_pick\"], [\"removetag\"] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(partition):\n",
    "    return partition.drop_duplicates(subset=[\"project_id\", \"tags\", \"lender_id\", \"loan_shareAmount\", \"loan_date\"])\n",
    "\n",
    "\n",
    "ds = ds.groupby(\"project_id\").apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tag_loans = ds.groupby(\"project_id\").tags.transform(\"nunique\") == 1\n",
    "one_tag_loans = one_tag_loans.rename(\"is_single_tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.merge(one_tag_loans.to_frame(), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[(ds[\"is_single_tag\"] == True) & (ds[\"tags\"] == \"removetag\")][\"tags\"] = \"empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[ds.tags != \"removetag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"tags\"] = ds[\"tags\"].astype(\"category\").cat.as_ordered()\n",
    "ds[\"tags\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anon_1 = ds.lender_name.str.lower().str.startswith(\"anonymous\")\n",
    "is_anon_2 = ds.lender_publicId.fillna(\"\").str.startswith(\"anon\")\n",
    "anons = is_anon_1 | is_anon_2\n",
    "print(\"anynomous Lenders\")\n",
    "# print some annons\n",
    "ds[anons][[\"lender_id\", \"lender_name\", \"lender_publicId\"]].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many anons out there?\n",
    "anons.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those anon\n",
    "ds = ds[~anons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_parquet(\"checkpoints/preprocessed_2023-08-28T11-09-39.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"checkpoints/preprocessed_2023-08-28T11-09-39.parquet\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag preprocessing with dask\n",
    "\n",
    "Why? BEcause without dask, the `drop_duplicates` could not work in the whole dataset \n",
    "even when using `subset=['project_id', 'lender_id', 'tags', 'shareAmount', 'date']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\"checkpoints/explodeddata.parquet\")\n",
    "# ddf = dd.read_parquet(\"checkpoints/explodeddata_sample_1000.parquet\")\n",
    "ddf = ddf.set_index(\"project_id\", drop=False)  # why? To easier for duplication\n",
    "ddf = ddf.repartition(npartitions=4)\n",
    "print(ddf.npartitions)\n",
    "# ddf = dask_cudf.from_dask_dataframe(ddf)\n",
    "# print(ddf.npartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill all project which have no tag by tag `empty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tag call `empty`\n",
    "ddf[\"tags\"] = ddf[\"tags\"].fillna(\"empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some tags \n",
    "The folowing tags should be remove, because it isn't visible to Users:  \n",
    "- `user_favorite`\n",
    "- `user_like`\n",
    "- `volunteer_like`\n",
    "- `volunteer_pick`\n",
    "\n",
    "If a project **only** have those tags, change all those tags into `empty`, then remove duplicate again.  \n",
    "If a project have other tags rather than those tags, just drop those tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (dds[\"tags\"] == \"user_favorite\").sum().compute(),\\\n",
    "# (dds[\"tags\"] == \"user_like\").sum().compute(),\\\n",
    "# (dds[\"tags\"] == \"volunteer_like\").sum().compute(),\\\n",
    "# (dds[\"tags\"] == \"volunteer_pick\").sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"tags\"] = ddf.tags.replace([\"user_favorite\", \"user_like\", \"volunteer_like\", \"volunteer_pick\"], [\"removetag\"] * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after turn those tags into `removetag`, Threre will be duplicates. Now remove them here.\n",
    "but turn out, the removal require a lot of memory. Hence, dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(partition):\n",
    "    return partition.drop_duplicates(subset=[\"project_id\", \"tags\", \"lender_id\", \"loan_shareAmount\", \"loan_date\"])\n",
    "\n",
    "\n",
    "df_1 = ddf.map_partitions(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is why we need to use dask\n",
    "# df_1 = df.drop_duplicates(subset=['project_id', 'tags', 'lender_id', 'shareAmount', 'date'], split_out=4)\n",
    "# df_1 = ddf.drop_duplicates(subset=[\"project_id\", \"tags\", \"lender_id\", \"loan_shareAmount\", \"loan_date\"])\n",
    "# df_1 = client.persist(df_1)\n",
    "# df_1 = df_1.repartition(npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_tag_loans = df_1.groupby(\"project_id\").tags.transform(\"nunique\", meta=(\"tags\", \"int\")) == 1\n",
    "one_tag_loans = one_tag_loans.rename(\"is_single_tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1.merge(one_tag_loans.to_frame(), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[(df_2[\"is_single_tag\"] == True) & (df_2[\"tags\"] == \"removetag\")][\"tags\"] = \"empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# count tags by projects (except tag nan, if exists)\n",
    "# in cudf, use nunique(drop=False)\n",
    "# dask do not support that `drop` yet\n",
    "# fortunately, the colummn tags contain no null value\n",
    "# one can confirm with `ds_2.tags.isna().sum().compute()`\n",
    "\n",
    "one_tag_loans = df_2.groupby(\"project_id\").tags.nunique() == 1  # count tags, except\n",
    "one_tag_loans = one_tag_loans[one_tag_loans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "one_tag_loans = one_tag_loans.index.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "should_change_tag = (df_2[\"project_id\"].isin(one_tag_loans)) & (df_2[\"tags\"] == \"removetag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df_2[should_change_tag].tags = \"empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2[df_2.tags != \"removetag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove no-tag Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = df_3[df_3.tags != \"empty\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert to `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[\"tags\"] = df_3[\"tags\"].astype(\"category\").cat.as_ordered()\n",
    "df_3[\"tags\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove anonymous Lenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lender is call *Annonymous* when:\n",
    "- `lender_name` (lower form) starts with \"anonymous\"\n",
    "- Or, `lender_publicId` starts with \"anon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anon_1 = df_4.lender_name.str.lower().str.startswith(\"anonymous\")\n",
    "is_anon_2 = df_4.lender_publicId.fillna(\"\").str.startswith(\"anon\")\n",
    "anons = is_anon_1 | is_anon_2\n",
    "print(\"anynomous Lenders\")\n",
    "# anons = df_2.lender_publicId.fillna(\"\").str.startswith(\"anon\")\n",
    "# print some annons\n",
    "# df_4[anons][[\"lender_id\", \"lender_name\", \"lender_publicId\"]].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those anon\n",
    "df_5 = df_4[~anons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to deduplicate one more time\n",
    "# df_6 = df_5.drop_duplicates(subset=[\"project_id\", \"tags\", \"lender_id\", \"loan_shareAmount\", \"loan_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_7 = df_6.reset_index(drop=True)\n",
    "df_7 = df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.to_parquet(\"checkpoints/preprocessed_2023-08-28T11-09-39_parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ads = cudf.read_parquet(\"checkpoints/vn_since_20200101.parquet\")\n",
    "# ads[\"sector_name\"] = ads[\"sector_name\"].astype(\"category\")\n",
    "# ads[\"geocode_country_name\"] = ads[\"geocode_country_name\"].astype(\"category\")\n",
    "# ads[\"activity_name\"] = ads[\"activity_name\"].astype(\"category\")\n",
    "# ads[\"tags\"] = ads[\"tags\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.loc[[9628, 1366545]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data for Vietnam only\n",
    "Filtering, only take `Vietnam` into account\n",
    "Why? Because there are a lot of rows and we try to localize the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn = df_7[df_7[\"geocode_country_name\"] == \"Vietnam\"]\n",
    "# Try to limit the timeline, because I am not yet comfortable to work with large data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_since_2018 = vn[vn.fundraisingDate >= \"2018-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_since_2018.compute().to_parquet(\"checkpoints/vn_since_20180101.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"geocode_country_name\"].value_counts()[\"Vietnam\"]\n",
    "# ds = ds[ds[\"geocode_country_name\"] == \"Vietnam\"]\n",
    "# # Try to limit the timeline, because I am not yet comfortable to work with large data\n",
    "# ds = ds[ds.fundraisingDate > \"2020-01-01\"]\n",
    "# \"the number of Loans (might duplicated) under investigation is\", len(ds)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudf.read_parquet(\"checkpoints/vn_since_20180101.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
