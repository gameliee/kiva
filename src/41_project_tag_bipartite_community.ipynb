{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = pd.read_csv(\"../data/gen/project_tags.csv\")\n",
    "PT.rename(columns={\":END_ID\": \"tag\"}, inplace=True)\n",
    "PT[\"project_id\"] = PT[\"project_id\"].astype(str)\n",
    "PT[\"tag\"] = PT[\"tag\"].astype(\"category\")\n",
    "PT.drop(columns=[\":TYPE\"], inplace=True)\n",
    "PT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "PT[\"V1\"] = le.fit_transform(PT[\"tag\"])\n",
    "PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "PT[\"V2\"] = le.fit_transform(PT[\"project_id\"]) + PT[\"V1\"].max() + 1\n",
    "PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of vertex\n",
    "vertex_count = PT[\"project_id\"].nunique() + PT[\"tag\"].nunique()\n",
    "assert PT[\"V1\"].nunique() == PT[\"tag\"].nunique()\n",
    "assert PT[\"V2\"].nunique() == PT[\"project_id\"].nunique()\n",
    "print(vertex_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with V1, V2, loan_amount columns\n",
    "PT[\"weight\"] = 1\n",
    "PT[[\"V1\", \"V2\", \"weight\"]].to_csv(\"checkpoints/project_tag_bipartite.csv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = PT[[\"V1\", \"tag\"]].drop_duplicates()\n",
    "dictionary.rename(columns={\"tag\": \"name\", \"V1\": \"id\"}, inplace=True)\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2 = PT[[\"V2\", \"project_id\"]].drop_duplicates()\n",
    "dictionary2.rename(columns={\"project_id\": \"name\", \"V2\": \"id\"}, inplace=True)\n",
    "dictionary2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat two dictionaries\n",
    "dictionary = pd.concat([dictionary, dictionary2])\n",
    "dictionary[\"name\"] = dictionary[\"name\"].astype(str)\n",
    "print(len(dictionary))\n",
    "assert len(dictionary) == vertex_count\n",
    "dictionary.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.sort_values(by=[\"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dictionary to file checkpoints/lender_tag_bipartite_Dictionary.txt, without header and index. The first column should be id\n",
    "dictionary[[\"id\", \"name\"]].to_csv(\n",
    "    \"checkpoints/project_tag_bipartite_Dictionary.txt\", sep=\"\\t\", header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run `biLouvain`\n",
    "\n",
    "```bash\n",
    "./biLouvain -d \"\\t\" -i ../src/checkpoints/project_tag_bipartite.csv -order 2\n",
    "```\n",
    "\n",
    "Reference: https://github.com/paolapesantez/biLouvain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export to Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse result file. The result file looks like this\n",
    "\n",
    "```text\n",
    "Community 0[V1]: 0\n",
    "Community 1[V1]: 1\n",
    "Community 2[V2]: 2,3\n",
    "Community 3[V2]: 4\n",
    "\n",
    "Singletons Partition V1: 2\n",
    "Singletons Partition V2: 1\n",
    "0,1,2,2,3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the last line of the file\n",
    "with open(\"checkpoints/project_tag_bipartite_ResultsCommunities.txt\", \"r\") as f:\n",
    "    result_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = r\"^Community (?P<community_id>\\d+)\\[V(?P<vertex_type>\\d+)\\]: (?P<vertexes>.*)$\"\n",
    "matches = re.finditer(regex, result_text, re.MULTILINE)\n",
    "\n",
    "community_result = []\n",
    "\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    community_id = match.group(\"community_id\")\n",
    "    vertex_type = match.group(\"vertex_type\")\n",
    "    vertexes = match.group(\"vertexes\").split(\", \")[0]\n",
    "    vertexes = vertexes.split(\",\")\n",
    "    print(community_id, vertex_type, vertexes)\n",
    "    for vertex in vertexes:\n",
    "        community_result.append([community_id, vertex_type, vertex])\n",
    "\n",
    "community_result = pd.DataFrame(community_result, columns=[\"community_id\", \"vertex_type\", \"vertex_name\"])\n",
    "community_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = dictionary.merge(community_result, left_on=\"name\", right_on=\"vertex_name\")\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create networkx graph and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(PT, \"project_id\", \"tag\")\n",
    "G.add_nodes_from(PT[\"project_id\"].drop_duplicates(), type=\"Project\")\n",
    "G.add_nodes_from(PT[\"tag\"].drop_duplicates(), type=\"Tag\")\n",
    "\n",
    "# refine node attributes with community id\n",
    "for row in nodes.itertuples():\n",
    "    G.nodes[row.name][\"community_id\"] = row.community_id\n",
    "\n",
    "print(G.number_of_nodes(), G.number_of_edges())\n",
    "nx.write_gexf(G, \"checkpoints/project_tag_bipartite_community.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
