{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LP = pd.read_csv(\"../data/gen/lender_project.csv\")\n",
    "LP.rename(columns={\":END_ID(Project-ID)\": \"project_id\", \":START_ID(Lender-ID)\": \"lender_id\"}, inplace=True)\n",
    "LP.drop(columns=[\":TYPE\", \"loan_date\"], inplace=True)\n",
    "LP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT = pd.read_csv(\"../data/gen/project_tags.csv\")\n",
    "PT.rename(columns={\":END_ID\": \"tag\"}, inplace=True)\n",
    "PT[\"tag\"] = PT[\"tag\"].astype(\"category\")\n",
    "PT.drop(columns=[\":TYPE\"], inplace=True)\n",
    "PT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(LP, PT, on=\"project_id\")\n",
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == merged.duplicated().sum()\n",
    "merged.drop(columns=[\"project_id\"], inplace=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LT = merged.groupby([\"lender_id\", \"tag\"]).agg({\"lender_publicId\": \"first\", \"loan_shareAmount\": \"sum\"}).reset_index()\n",
    "LT.rename(columns={\"loan_shareAmount\": \"loan_amount\"}, inplace=True)\n",
    "LT = LT[LT[\"loan_amount\"] > 0]\n",
    "LT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "LT[\"V1\"] = le.fit_transform(LT[\"lender_id\"])\n",
    "LT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "LT[\"V2\"] = le.fit_transform(LT[\"tag\"]) + LT[\"V1\"].max() + 1\n",
    "LT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of vertex\n",
    "vertex_count = LT[\"lender_id\"].nunique() + LT[\"tag\"].nunique()\n",
    "assert vertex_count == LT[\"V1\"].nunique() + LT[\"V2\"].nunique()\n",
    "print(vertex_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with V1, V2, loan_amount columns\n",
    "LT[[\"V1\", \"V2\", \"loan_amount\"]].to_csv(\"checkpoints/lender_tag_bipartite.csv\", sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = LT[[\"V1\", \"lender_publicId\"]].drop_duplicates()\n",
    "dictionary.rename(columns={\"lender_publicId\": \"name\", \"V1\": \"id\"}, inplace=True)\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2 = LT[[\"V2\", \"tag\"]].drop_duplicates()\n",
    "dictionary2.rename(columns={\"tag\": \"name\", \"V2\": \"id\"}, inplace=True)\n",
    "dictionary2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat two dictionaries\n",
    "dictionary = pd.concat([dictionary, dictionary2])\n",
    "print(len(dictionary))\n",
    "assert len(dictionary) == vertex_count\n",
    "dictionary.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dictionary to file checkpoints/lender_tag_bipartite_Dictionary.txt, without header and index. The first column should be id\n",
    "dictionary[[\"id\", \"name\"]].to_csv(\n",
    "    \"checkpoints/lender_tag_bipartite_Dictionary.txt\", sep=\"\\t\", header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run `biLouvain`\n",
    "\n",
    "```bash\n",
    "./biLouvain -d \"\\t\" -i ../src/checkpoints/lender_tag_bipartite.csv  \n",
    "```\n",
    "\n",
    "Reference: https://github.com/paolapesantez/biLouvain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export to Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse result file. The result file looks like this\n",
    "\n",
    "```text\n",
    "Community 0[V1]: 0\n",
    "Community 1[V1]: 1\n",
    "Community 2[V2]: 2,3\n",
    "Community 3[V2]: 4\n",
    "\n",
    "Singletons Partition V1: 2\n",
    "Singletons Partition V2: 1\n",
    "0,1,2,2,3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the last line of the file\n",
    "with open(\"checkpoints/lender_tag_bipartite_ResultsCommunities.txt\", \"r\") as f:\n",
    "    result_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "# the above tag defines encoding for this document and is for Python 2.x compatibility\n",
    "\n",
    "import re\n",
    "\n",
    "regex = r\"^Community (?P<community_id>\\d+)\\[V(?P<vertex_type>\\d+)\\]: (?P<vertexes>.*)$\"\n",
    "matches = re.finditer(regex, result_text, re.MULTILINE)\n",
    "\n",
    "community_result = []\n",
    "\n",
    "for matchNum, match in enumerate(matches, start=1):\n",
    "    community_id = match.group(\"community_id\")\n",
    "    vertex_type = match.group(\"vertex_type\")\n",
    "    vertexes = match.group(\"vertexes\").split(\", \")[0]\n",
    "    vertexes = vertexes.split(\",\")\n",
    "    for vertex in vertexes:\n",
    "        community_result.append([community_id, vertex_type, vertex])\n",
    "\n",
    "community_result = pd.DataFrame(community_result, columns=[\"community_id\", \"vertex_type\", \"vertex\"])\n",
    "community_result[\"vertex\"] = community_result[\"vertex\"].astype(int)\n",
    "community_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.merge(community_result, left_on=\"id\", right_on=\"vertex\").drop(columns=[\"id\"])\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create networkx graph and show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(LT, \"lender_publicId\", \"tag\", \"loan_amount\")\n",
    "G.add_nodes_from(LT[\"lender_publicId\"].drop_duplicates(), type=\"Lender\")\n",
    "G.add_nodes_from(LT[\"tag\"].drop_duplicates(), type=\"Tag\")\n",
    "\n",
    "# refine node attributes with community id\n",
    "for row in dictionary.itertuples():\n",
    "    G.nodes[row.name][\"community_id\"] = row.community_id\n",
    "\n",
    "G.number_of_nodes(), G.number_of_edges()\n",
    "nx.write_gexf(G, \"checkpoints/lender_tag_bipartite_community.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
